{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Film Development Time Prediction\n",
    "\n",
    "End-to-end orchestration of the pipeline:\n",
    "\n",
    "1. Data loading & exploration\n",
    "2. Frozen train/val/test splits\n",
    "3. Feature engineering walkthrough (slopes → latent vectors → aggregates)\n",
    "4. 5-fold cross-validation with ExtraTreesRegressor\n",
    "5. Final model: train on all trainval, evaluate on held-out test\n",
    "6. Feature importance & error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from data_split import DataSplitter, prepare_data\n",
    "from film_slopes import FilmSlopeEstimator, DevDilSlopeEstimator, DevDilutionSlopeEstimator\n",
    "from latent_features import LatentFeatureBuilder\n",
    "from aggregate_features import AggregateFeatureBuilder\n",
    "from train_extratrees import build_features, smape, evaluate, DROP_COLS, TARGET\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'figure.dpi': 110, 'axes.spines.top': False, 'axes.spines.right': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Filtering pipeline\")\n",
    "print(\"=\" * 60)\n",
    "df = prepare_data(\"film_data.csv\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Final dataset\n",
    "  Rows        : {len(df):,}\n",
    "  Films       : {df['Film'].nunique():,}\n",
    "  Developers  : {df['Developer'].nunique():,}\n",
    "  dev_dils    : {df['dev_dil'].nunique():,}\n",
    "  Time range  : {df['35mm'].min():.1f} – {df['35mm'].max():.1f} min\n",
    "  ISO range   : {df['iso'].min():.0f} – {df['iso'].max():.0f}\n",
    "\"\"\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Development time distribution\n",
    "axes[0].hist(df['35mm'], bins=40, color='steelblue', edgecolor='white', linewidth=0.4)\n",
    "axes[0].set_xlabel('Development time (min)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Development time distribution')\n",
    "axes[0].axvline(df['35mm'].median(), color='tomato', linestyle='--', label=f\"median {df['35mm'].median():.1f} min\")\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# ISO distribution\n",
    "iso_counts = np.log2(df['iso']).round(1)\n",
    "axes[1].hist(iso_counts, bins=30, color='seagreen', edgecolor='white', linewidth=0.4)\n",
    "axes[1].set_xlabel('log₂(ISO)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('ISO distribution (log₂ scale)')\n",
    "\n",
    "# Top 15 films by row count\n",
    "top_films = df['Film'].value_counts().head(15)\n",
    "axes[2].barh(top_films.index[::-1], top_films.values[::-1], color='mediumpurple')\n",
    "axes[2].set_xlabel('Row count')\n",
    "axes[2].set_title('Top 15 films by row count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stops (push/pull) distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(df['stops'], bins=40, color='darkorange', edgecolor='white', linewidth=0.4)\n",
    "axes[0].set_xlabel('Stops from box ISO (log₂)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Push/pull distribution')\n",
    "axes[0].axvline(0, color='black', linestyle=':', linewidth=1, label='box ISO')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# Time vs stops scatter (sample)\n",
    "sample = df.sample(min(2000, len(df)), random_state=0)\n",
    "sc = axes[1].scatter(sample['stops'], sample['35mm'], c=np.log2(sample['iso']),\n",
    "                     cmap='plasma', alpha=0.35, s=8, linewidths=0)\n",
    "plt.colorbar(sc, ax=axes[1], label='log₂(ISO)')\n",
    "axes[1].set_xlabel('Stops from box ISO')\n",
    "axes[1].set_ylabel('Development time (min)')\n",
    "axes[1].set_title('Time vs push/pull (sample of 2000)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Val/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = DataSplitter.load(\"splits.json\")\n",
    "\n",
    "print(f\"Seed          : {splitter.seed}\")\n",
    "print(f\"N folds       : {splitter.n_folds}\")\n",
    "print(f\"Test fraction : {splitter.test_fraction}\")\n",
    "print(f\"Test rows     : {len(splitter.test_indices):,}\")\n",
    "print(f\"Trainval rows : {len(splitter.get_trainval_indices()):,}\")\n",
    "print()\n",
    "\n",
    "summary = splitter.summary(df)\n",
    "display(summary[['set', 'n_rows', 'pct_total', 'n_films', 'n_dev_dils',\n",
    "                  'time_median', 'time_mean', 'time_std']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no leakage\n",
    "test_set = set(splitter.test_indices)\n",
    "all_folds = [set(f) for f in splitter.fold_indices]\n",
    "\n",
    "for i, fold in enumerate(all_folds):\n",
    "    assert len(test_set & fold) == 0, f\"Test overlaps fold {i}\"\n",
    "for i in range(len(all_folds)):\n",
    "    for j in range(i + 1, len(all_folds)):\n",
    "        assert len(all_folds[i] & all_folds[j]) == 0, f\"Fold {i} overlaps fold {j}\"\n",
    "\n",
    "all_assigned = test_set.copy()\n",
    "for fold in all_folds:\n",
    "    all_assigned |= fold\n",
    "assert len(all_assigned) == len(df)\n",
    "\n",
    "print(\"Split integrity checks passed:\")\n",
    "print(\"  No test/fold overlap\")\n",
    "print(\"  No fold/fold overlap\")\n",
    "print(f\"  All {len(df):,} rows accounted for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering Walkthrough (Fold 0)\n",
    "\n",
    "All transformers are fit on the **training fold only** to prevent leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Slope Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = splitter.get_fold(0)\n",
    "df_train0 = df.loc[train_idx]\n",
    "df_val0   = df.loc[val_idx]\n",
    "\n",
    "# Fit all three slope estimators on training fold\n",
    "film_est  = FilmSlopeEstimator().fit(df_train0)\n",
    "devdil_est = DevDilSlopeEstimator(box_isos=film_est.box_isos).fit(df_train0)\n",
    "dil_est   = DevDilutionSlopeEstimator().fit(df_train0)\n",
    "\n",
    "print(f\"FilmSlopeEstimator     : {len(film_est.film_slopes):,} films with specific slope  \"\n",
    "      f\"(global={film_est.global_slope:.4f})\")\n",
    "print(f\"DevDilSlopeEstimator   : {len(devdil_est.dev_dil_slopes):,} dev_dils with specific slope  \"\n",
    "      f\"(global={devdil_est.global_slope:.4f})\")\n",
    "print(f\"DevDilutionSlopeEstimator: {len(dil_est.dev_dilution_slopes):,} developers with dilution slope  \"\n",
    "      f\"(global={dil_est.global_slope:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Film slope summary — top 10 most-observed films\n",
    "fs = film_est.summary().sort_values('n_devs', ascending=False)\n",
    "print(\"Top 10 films by developer count:\")\n",
    "display(fs[['Film','slope','accel','se','r2_within','n_obs','n_devs','box_iso']].head(10).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise film slope distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(fs['slope'], bins=30, color='steelblue', edgecolor='white', linewidth=0.4)\n",
    "axes[0].axvline(film_est.global_slope, color='tomato', linestyle='--',\n",
    "                label=f'global slope={film_est.global_slope:.3f}')\n",
    "axes[0].set_xlabel('Film push/pull slope β₁')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Per-film push/pull slope distribution')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "axes[1].scatter(fs['slope'], fs['accel'], alpha=0.5, s=20, color='seagreen')\n",
    "axes[1].set_xlabel('Linear slope β₁')\n",
    "axes[1].set_ylabel('Acceleration β₂')\n",
    "axes[1].set_title('Slope vs acceleration per film')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise a well-known film's push/pull curve\n",
    "candidate_films = [f for f in ['Ilford HP5 Plus 400', 'Kodak Tri-X 400', 'Ilford Delta 400 Professional',\n",
    "                                'Kodak T-Max 400'] if f in film_est.film_slopes]\n",
    "if candidate_films:\n",
    "    film_name = candidate_films[0]\n",
    "    info = film_est.film_slopes[film_name]\n",
    "    stops_range = np.linspace(-2, 3, 80)\n",
    "    log_ratio = info['slope'] * stops_range + info['accel'] * stops_range**2\n",
    "\n",
    "    # Raw data points\n",
    "    film_rows = df_train0[df_train0['Film'] == film_name]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.scatter(film_rows['stops'], np.log(film_rows['35mm']), alpha=0.4, s=15,\n",
    "               color='steelblue', label='training data (log time)')\n",
    "    ax.plot(stops_range, log_ratio + np.log(film_rows['35mm'].median()), color='tomato',\n",
    "            linewidth=2, label=f'quadratic fit (β₁={info[\"slope\"]:.3f}, β₂={info[\"accel\"]:.3f})')\n",
    "    ax.set_xlabel('Stops from box ISO')\n",
    "    ax.set_ylabel('log(time)')\n",
    "    ax.set_title(f'Push/pull curve: {film_name}')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.axvline(0, color='black', linestyle=':', linewidth=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"None of the target films found in fold 0 training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Matrix Factorization (Latent Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv = LatentFeatureBuilder(n_factors=30, reg=0.005)\n",
    "lv_train0 = lv.fit_transform(df_train0)\n",
    "lv_val0   = lv.transform(df_val0)\n",
    "\n",
    "info = lv.summary()\n",
    "print(f\"Matrix shape  : {info['matrix_shape'][0]} dev_dils × {info['matrix_shape'][1]} film@ISO columns\")\n",
    "print(f\"Matrix density: {info['density']:.1%}\")\n",
    "print(f\"Filled cells  : {info['filled_cells']:,}\")\n",
    "print(f\"Recon SMAPE   : {info['recon_smape']:.2f}%\")\n",
    "print(f\"Recon R²      : {info['recon_r2']:.4f}\")\n",
    "print(f\"Films with LV : {info['n_films_with_lv']}\")\n",
    "print(f\"Dev_dils w/ LV: {info['n_dd_with_lv']}\")\n",
    "print(f\"Global mean   : {info['global_mean_time']:.2f} min\")\n",
    "print(f\"\\nLV columns ({lv_train0.shape[1]}):\")\n",
    "print(', '.join(lv_train0.columns[:6].tolist()), '...',\n",
    "      ', '.join(lv_train0.columns[-3:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage on val\n",
    "val_cov = lv_val0['lv_available'].mean()\n",
    "print(f\"Val LV coverage : {val_cov:.1%} ({lv_val0['lv_available'].sum()} / {len(lv_val0)} rows)\")\n",
    "\n",
    "# MF-only SMAPE on covered val rows\n",
    "mask = lv_val0['lv_available']\n",
    "if mask.sum() > 0:\n",
    "    actual    = df_val0.loc[mask, '35mm'].values\n",
    "    predicted = lv_val0.loc[mask, 'lv_pred_time'].values\n",
    "    mf_smape  = np.mean(2 * np.abs(actual - predicted) / (np.abs(actual) + np.abs(predicted))) * 100\n",
    "    mf_mae    = np.mean(np.abs(actual - predicted))\n",
    "    print(f\"MF-only val SMAPE: {mf_smape:.2f}%  MAE: {mf_mae:.2f} min  (on {mask.sum()} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = AggregateFeatureBuilder()\n",
    "agg_train0 = agg.fit_transform(df_train0)\n",
    "agg_val0   = agg.transform(df_val0)\n",
    "\n",
    "s = agg.summary()\n",
    "print(f\"Entity levels   : {s['entity_levels']}  (film, film@ISO, dev_dil, developer)\")\n",
    "print(f\"Stats per entity: {s['features_per_entity']}\")\n",
    "print(f\"Row features    : {s['row_features']}\")\n",
    "print(f\"Total features  : {s['total_features']}\")\n",
    "print(f\"\\nEntities seen in training:\")\n",
    "print(f\"  Films     : {s['n_films']}\")\n",
    "print(f\"  Film@ISO  : {s['n_film_iso']}\")\n",
    "print(f\"  Dev_dils  : {s['n_dev_dils']}\")\n",
    "print(f\"  Developers: {s['n_developers']}\")\n",
    "\n",
    "# Coverage\n",
    "for prefix, label in [('film_agg', 'Film'), ('film_iso_agg', 'Film@ISO'),\n",
    "                       ('dd_agg', 'Dev_dil'), ('dev_agg', 'Developer')]:\n",
    "    col = f'{prefix}_count'\n",
    "    cov = agg_val0[col].notna().mean() * 100\n",
    "    print(f\"  Val coverage {label:10s}: {cov:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Combined Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0, y_train0, X_val0, y_val0, feature_names = build_features(df_train0, df_val0)\n",
    "\n",
    "print(f\"X_train shape : {X_train0.shape}\")\n",
    "print(f\"X_val shape   : {X_val0.shape}\")\n",
    "print(f\"\\nNaN fraction per split:\")\n",
    "print(f\"  train : {np.isnan(X_train0).mean():.2%}\")\n",
    "print(f\"  val   : {np.isnan(X_val0).mean():.2%}\")\n",
    "print(f\"\\nFirst 10 feature names: {feature_names[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"5-Fold Cross-Validation — ExtraTreesRegressor\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold in range(splitter.n_folds):\n",
    "    train_idx, val_idx = splitter.get_fold(fold)\n",
    "    df_train = df.loc[train_idx]\n",
    "    df_val   = df.loc[val_idx]\n",
    "\n",
    "    t0 = time.time()\n",
    "    X_train, y_train, X_val, y_val, feat_names = build_features(df_train, df_val)\n",
    "    t_feat = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    et = ExtraTreesRegressor(\n",
    "        n_estimators=500, max_features=0.5,\n",
    "        min_samples_leaf=2, random_state=42, n_jobs=-1,\n",
    "    )\n",
    "    et.fit(X_train, y_train)\n",
    "    t_train = time.time() - t0\n",
    "\n",
    "    y_pred_train = et.predict(X_train)\n",
    "    y_pred_val   = et.predict(X_val)\n",
    "\n",
    "    print(f\"\\nFold {fold}  (feat: {t_feat:.0f}s  train: {t_train:.0f}s  features: {X_train.shape[1]})\")\n",
    "    evaluate(y_train, y_pred_train, 'train')\n",
    "    res = evaluate(y_val, y_pred_val, 'val')\n",
    "    fold_results.append({'fold': fold, **res, 'n_val': len(y_val)})\n",
    "\n",
    "print()\n",
    "cv_df = pd.DataFrame(fold_results)\n",
    "print(\"=\" * 70)\n",
    "print(\"CV Summary\")\n",
    "print(\"=\" * 70)\n",
    "for metric in ['smape', 'mae', 'mape', 'r2']:\n",
    "    vals = cv_df[metric]\n",
    "    print(f\"  {metric:8s}  mean={vals.mean():.4f}  std={vals.std():.4f}  \"\n",
    "          f\"min={vals.min():.4f}  max={vals.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise CV results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "folds = cv_df['fold'].values\n",
    "\n",
    "axes[0].bar(folds, cv_df['smape'], color='steelblue', alpha=0.8)\n",
    "axes[0].axhline(cv_df['smape'].mean(), color='tomato', linestyle='--',\n",
    "                label=f\"mean {cv_df['smape'].mean():.2f}%\")\n",
    "axes[0].set_xticks(folds)\n",
    "axes[0].set_xlabel('Fold')\n",
    "axes[0].set_ylabel('SMAPE (%)')\n",
    "axes[0].set_title('Validation SMAPE per fold')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "axes[1].bar(folds, cv_df['mae'], color='seagreen', alpha=0.8)\n",
    "axes[1].axhline(cv_df['mae'].mean(), color='tomato', linestyle='--',\n",
    "                label=f\"mean {cv_df['mae'].mean():.2f} min\")\n",
    "axes[1].set_xticks(folds)\n",
    "axes[1].set_xlabel('Fold')\n",
    "axes[1].set_ylabel('MAE (min)')\n",
    "axes[1].set_title('Validation MAE per fold')\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Model — Train on All Trainval, Evaluate on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_idx = splitter.get_trainval_indices()\n",
    "test_idx     = splitter.test_indices\n",
    "\n",
    "df_trainval = df.loc[trainval_idx]\n",
    "df_test     = df.loc[test_idx]\n",
    "\n",
    "t0 = time.time()\n",
    "X_tv, y_tv, X_test, y_test, final_feature_names = build_features(df_trainval, df_test)\n",
    "print(f\"Feature build time: {time.time() - t0:.1f}s\")\n",
    "\n",
    "t0 = time.time()\n",
    "et_final = ExtraTreesRegressor(\n",
    "    n_estimators=500, max_features=0.5,\n",
    "    min_samples_leaf=2, random_state=42, n_jobs=-1,\n",
    ")\n",
    "et_final.fit(X_tv, y_tv)\n",
    "print(f\"Training time     : {time.time() - t0:.1f}s\")\n",
    "\n",
    "y_pred_tv   = et_final.predict(X_tv)\n",
    "y_pred_test = et_final.predict(X_test)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"Final evaluation\")\n",
    "print(\"=\" * 70)\n",
    "evaluate(y_tv,   y_pred_tv,   'trainval')\n",
    "evaluate(y_test, y_pred_test, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.Series(et_final.feature_importances_, index=final_feature_names).sort_values(ascending=False)\n",
    "top30 = imp.head(30)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = ['#1f77b4' if 'lv_' in n else\n",
    "          '#2ca02c' if any(x in n for x in ['slope', 'accel', 'dil']) else\n",
    "          '#ff7f0e' for n in top30.index]\n",
    "ax.barh(top30.index[::-1], top30.values[::-1], color=colors[::-1])\n",
    "ax.set_xlabel('Feature importance')\n",
    "ax.set_title('Top 30 features by importance')\n",
    "\n",
    "# Legend patches\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ca02c', label='Slope features'),\n",
    "    Patch(facecolor='#1f77b4', label='Latent vector features'),\n",
    "    Patch(facecolor='#ff7f0e', label='Aggregate features'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, fontsize=9, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Cumulative importance — top 10: {top30.head(10).sum()*100:.1f}%  \"\n",
    "      f\"top 30: {top30.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance breakdown by feature group\n",
    "groups = {\n",
    "    'Latent vectors': [n for n in final_feature_names if n.startswith('lv_')],\n",
    "    'Slope features': [n for n in final_feature_names\n",
    "                       if any(x in n for x in ['slope', 'accel', 'pred_log_ratio'])],\n",
    "    'Aggregate features': [n for n in final_feature_names\n",
    "                           if any(x in n for x in ['_agg_', 'dil_factor', 'is_box_iso'])],\n",
    "    'Other': [],\n",
    "}\n",
    "covered = set()\n",
    "for k, v in groups.items():\n",
    "    covered |= set(v)\n",
    "groups['Other'] = [n for n in final_feature_names if n not in covered]\n",
    "\n",
    "print(\"Feature group importances:\")\n",
    "for group, names in groups.items():\n",
    "    total = imp[names].sum() * 100\n",
    "    print(f\"  {group:22s}: {len(names):3d} features  {total:5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis on test set\n",
    "residuals = y_pred_test - y_test\n",
    "smape_per_row = 2 * np.abs(y_pred_test - y_test) / (np.abs(y_pred_test) + np.abs(y_test)) * 100\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
    "\n",
    "# Actual vs predicted\n",
    "mn, mx = min(y_test.min(), y_pred_test.min()), max(y_test.max(), y_pred_test.max())\n",
    "axes[0, 0].scatter(y_test, y_pred_test, alpha=0.3, s=8, color='steelblue', linewidths=0)\n",
    "axes[0, 0].plot([mn, mx], [mn, mx], 'r--', linewidth=1)\n",
    "axes[0, 0].set_xlabel('Actual time (min)')\n",
    "axes[0, 0].set_ylabel('Predicted time (min)')\n",
    "axes[0, 0].set_title('Actual vs Predicted (test)')\n",
    "\n",
    "# Residual distribution\n",
    "axes[0, 1].hist(residuals, bins=50, color='seagreen', edgecolor='white', linewidth=0.3)\n",
    "axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0, 1].set_xlabel('Residual (pred − actual, min)')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title(f'Residual distribution  (mean={residuals.mean():.2f}, std={residuals.std():.2f})')\n",
    "\n",
    "# SMAPE vs actual time\n",
    "axes[1, 0].scatter(y_test, smape_per_row, alpha=0.3, s=8, color='darkorange', linewidths=0)\n",
    "axes[1, 0].axhline(smape_per_row.mean(), color='red', linestyle='--', linewidth=1,\n",
    "                   label=f'mean {smape_per_row.mean():.1f}%')\n",
    "axes[1, 0].set_xlabel('Actual time (min)')\n",
    "axes[1, 0].set_ylabel('Per-row SMAPE (%)')\n",
    "axes[1, 0].set_title('Error vs actual time')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "\n",
    "# SMAPE CDF\n",
    "sorted_smape = np.sort(smape_per_row)\n",
    "cdf = np.arange(1, len(sorted_smape) + 1) / len(sorted_smape)\n",
    "axes[1, 1].plot(sorted_smape, cdf, color='mediumpurple', linewidth=1.5)\n",
    "axes[1, 1].axvline(smape_per_row.mean(), color='red', linestyle='--', linewidth=1,\n",
    "                   label=f'mean {smape_per_row.mean():.1f}%')\n",
    "for pct, val in [(50, np.percentile(smape_per_row, 50)),\n",
    "                 (90, np.percentile(smape_per_row, 90))]:\n",
    "    axes[1, 1].axvline(val, color='gray', linestyle=':', linewidth=1,\n",
    "                       label=f'p{pct}={val:.1f}%')\n",
    "axes[1, 1].set_xlabel('SMAPE (%)')\n",
    "axes[1, 1].set_ylabel('CDF')\n",
    "axes[1, 1].set_title('SMAPE CDF (test)')\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst predictions\n",
    "test_rows = df_test[['Film', 'Developer', 'Dilution', 'ASA/ISO', '35mm']].copy()\n",
    "test_rows['predicted'] = y_pred_test.round(2)\n",
    "test_rows['smape']     = smape_per_row.round(2)\n",
    "test_rows['residual']  = residuals.round(2)\n",
    "\n",
    "print(\"Worst 15 predictions by SMAPE (test set):\")\n",
    "display(test_rows.sort_values('smape', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error breakdown by stops bucket\n",
    "stops_test = df_test['stops'].values\n",
    "bins = [-np.inf, -1.5, -0.5, 0.5, 1.5, 2.5, np.inf]\n",
    "labels = ['< -1.5', '-1.5–-0.5', '-0.5–0.5', '0.5–1.5', '1.5–2.5', '> 2.5']\n",
    "bucket = pd.cut(stops_test, bins=bins, labels=labels)\n",
    "\n",
    "bucket_df = pd.DataFrame({\n",
    "    'bucket': bucket,\n",
    "    'smape': smape_per_row,\n",
    "    'mae': np.abs(residuals),\n",
    "    'n': 1,\n",
    "})\n",
    "\n",
    "summary = bucket_df.groupby('bucket', observed=True).agg(\n",
    "    n=('n', 'sum'),\n",
    "    smape_mean=('smape', 'mean'),\n",
    "    smape_median=('smape', 'median'),\n",
    "    mae_mean=('mae', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "print(\"Error breakdown by push/pull bucket:\")\n",
    "display(summary.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance summary\n",
    "cv_smape_mean = cv_df['smape'].mean()\n",
    "cv_smape_std  = cv_df['smape'].std()\n",
    "test_smape    = smape(y_test, y_pred_test)\n",
    "test_mae      = np.mean(np.abs(y_test - y_pred_test))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Final Performance Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  CV SMAPE  : {cv_smape_mean:.2f}% ± {cv_smape_std:.2f}%\")\n",
    "print(f\"  Test SMAPE: {test_smape:.2f}%\")\n",
    "print(f\"  Test MAE  : {test_mae:.2f} min\")\n",
    "print(f\"  Features  : {X_tv.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
